# OpenRouter Skill

> Patterns for working with OpenRouter LLM API.

---

## Setup

### API Key
```bash
# .env
OPENROUTER_API_KEY=sk-or-v1-xxxxx
```

### Base Configuration
```typescript
const OPENROUTER_BASE_URL = 'https://openrouter.ai/api/v1';

const headers = {
  'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
  'HTTP-Referer': 'https://your-app.com',  // Required
  'X-Title': 'Your App Name',               // Optional, shows in dashboard
  'Content-Type': 'application/json',
};
```

---

## Basic Request

### Chat Completion
```typescript
const response = await fetch(`${OPENROUTER_BASE_URL}/chat/completions`, {
  method: 'POST',
  headers,
  body: JSON.stringify({
    model: 'anthropic/claude-3.5-sonnet',
    messages: [
      { role: 'system', content: 'You are a helpful assistant.' },
      { role: 'user', content: 'Hello!' }
    ],
  }),
});

const data = await response.json();
const reply = data.choices[0].message.content;
```

### With Parameters
```typescript
body: JSON.stringify({
  model: 'anthropic/claude-3.5-sonnet',
  messages: [...],
  max_tokens: 1000,
  temperature: 0.7,
  top_p: 0.9,
  stream: false,
})
```

---

## Streaming

### Basic Stream
```typescript
const response = await fetch(`${OPENROUTER_BASE_URL}/chat/completions`, {
  method: 'POST',
  headers,
  body: JSON.stringify({
    model: 'anthropic/claude-3.5-sonnet',
    messages: [...],
    stream: true,
  }),
});

const reader = response.body?.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  const chunk = decoder.decode(value);
  const lines = chunk.split('\n').filter(line => line.startsWith('data: '));

  for (const line of lines) {
    const data = line.replace('data: ', '');
    if (data === '[DONE]') continue;

    const parsed = JSON.parse(data);
    const content = parsed.choices[0]?.delta?.content || '';
    process.stdout.write(content);
  }
}
```

### Stream with Callback
```typescript
async function streamChat(
  messages: Message[],
  onChunk: (content: string) => void,
  onDone: () => void
) {
  const response = await fetch(`${OPENROUTER_BASE_URL}/chat/completions`, {
    method: 'POST',
    headers,
    body: JSON.stringify({
      model: 'anthropic/claude-3.5-sonnet',
      messages,
      stream: true,
    }),
  });

  const reader = response.body?.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value);
    for (const line of chunk.split('\n')) {
      if (!line.startsWith('data: ')) continue;
      const data = line.slice(6);
      if (data === '[DONE]') continue;

      const parsed = JSON.parse(data);
      const content = parsed.choices[0]?.delta?.content;
      if (content) onChunk(content);
    }
  }
  onDone();
}
```

---

## Models

### Top Models (2024-2025)
| Model | ID | Best For |
|-------|-----|----------|
| Claude 3.5 Sonnet | `anthropic/claude-3.5-sonnet` | Coding, analysis |
| Claude 3 Opus | `anthropic/claude-3-opus` | Complex reasoning |
| GPT-4 Turbo | `openai/gpt-4-turbo` | General purpose |
| GPT-4o | `openai/gpt-4o` | Fast, multimodal |
| Llama 3.1 405B | `meta-llama/llama-3.1-405b-instruct` | Open source, large |
| Mistral Large | `mistralai/mistral-large` | European, fast |
| Gemini Pro | `google/gemini-pro` | Google ecosystem |

### Get Available Models
```typescript
const response = await fetch(`${OPENROUTER_BASE_URL}/models`, {
  headers: { 'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}` }
});
const { data } = await response.json();
// data = array of model objects with pricing, context length, etc.
```

### Model Selection Helper
```typescript
const MODELS = {
  fast: 'anthropic/claude-3-haiku',
  balanced: 'anthropic/claude-3.5-sonnet',
  powerful: 'anthropic/claude-3-opus',
  cheap: 'meta-llama/llama-3.1-8b-instruct',
} as const;

function selectModel(task: 'fast' | 'balanced' | 'powerful' | 'cheap') {
  return MODELS[task];
}
```

---

## Error Handling

### Response Errors
```typescript
const response = await fetch(...);

if (!response.ok) {
  const error = await response.json();

  switch (response.status) {
    case 400:
      throw new Error(`Bad request: ${error.message}`);
    case 401:
      throw new Error('Invalid API key');
    case 402:
      throw new Error('Insufficient credits');
    case 429:
      throw new Error('Rate limited â€” retry after delay');
    case 503:
      throw new Error('Model temporarily unavailable');
    default:
      throw new Error(`API error: ${response.status}`);
  }
}
```

### Retry Logic
```typescript
async function fetchWithRetry(
  url: string,
  options: RequestInit,
  maxRetries = 3
) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      const response = await fetch(url, options);
      if (response.status === 429) {
        const delay = Math.pow(2, i) * 1000; // Exponential backoff
        await new Promise(r => setTimeout(r, delay));
        continue;
      }
      return response;
    } catch (error) {
      if (i === maxRetries - 1) throw error;
    }
  }
}
```

---

## Cost Management

### Check Credits
```typescript
const response = await fetch('https://openrouter.ai/api/v1/auth/key', {
  headers: { 'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}` }
});
const { data } = await response.json();
console.log(`Credits remaining: $${data.limit - data.usage}`);
```

### Track Usage Per Request
```typescript
const data = await response.json();

// Usage info in response
const usage = data.usage;
console.log(`Tokens: ${usage.prompt_tokens} in, ${usage.completion_tokens} out`);
console.log(`Cost: $${data.usage.total_cost || 'N/A'}`);
```

### Cost-Effective Patterns
```typescript
// 1. Use cheaper models for simple tasks
const model = isSimpleTask ? 'meta-llama/llama-3.1-8b-instruct' : 'anthropic/claude-3.5-sonnet';

// 2. Limit max_tokens
max_tokens: 500,  // Don't request more than needed

// 3. Cache responses
const cacheKey = hash(messages);
const cached = await cache.get(cacheKey);
if (cached) return cached;
```

---

## Advanced Patterns

### System Prompt Template
```typescript
const systemPrompt = `You are an AI assistant.

RULES:
- Be concise
- Use markdown formatting
- If unsure, say so

CONTEXT:
${context}

OUTPUT FORMAT:
${formatInstructions}`;
```

### Function Calling (Tools)
```typescript
body: JSON.stringify({
  model: 'openai/gpt-4-turbo',
  messages: [...],
  tools: [
    {
      type: 'function',
      function: {
        name: 'get_weather',
        description: 'Get current weather for a location',
        parameters: {
          type: 'object',
          properties: {
            location: { type: 'string', description: 'City name' },
          },
          required: ['location'],
        },
      },
    },
  ],
  tool_choice: 'auto',
})
```

### Multi-Model Fallback
```typescript
const FALLBACK_MODELS = [
  'anthropic/claude-3.5-sonnet',
  'openai/gpt-4-turbo',
  'meta-llama/llama-3.1-70b-instruct',
];

async function chatWithFallback(messages: Message[]) {
  for (const model of FALLBACK_MODELS) {
    try {
      return await chat(model, messages);
    } catch (error) {
      console.warn(`${model} failed, trying next...`);
    }
  }
  throw new Error('All models failed');
}
```

---

## TypeScript Types

```typescript
interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface ChatRequest {
  model: string;
  messages: Message[];
  max_tokens?: number;
  temperature?: number;
  top_p?: number;
  stream?: boolean;
  tools?: Tool[];
}

interface ChatResponse {
  id: string;
  model: string;
  choices: {
    message: Message;
    finish_reason: string;
  }[];
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

interface StreamDelta {
  choices: {
    delta: { content?: string };
    finish_reason: string | null;
  }[];
}
```

---

## Avoid

- Hardcoding API keys in code
- Not handling rate limits (429)
- Ignoring token costs
- Using expensive models for simple tasks
- Missing error handling on streams
- Not setting HTTP-Referer header (required!)
