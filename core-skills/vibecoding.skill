# VibeCoding Methodology Skill

> Context Engineering First - Developer as orchestrator, AI as generator.

---

## Core Concept

### Traditional vs VibeCoding Mindset
```
Old: "How do I write code?"
New: "How do I curate context?"
```

### Triadic Relationship
```
Developer (Orchestra Leader)
    ‚Üï
PROJECT.md (Context Hub)
    ‚Üï
AI Agent (Code Generator)
```

**Key Insight:** Successful VibeCoding depends on systematic context engineering, not just AI capability.

---

## Three Development Models

### Level 1: Context-Enhanced Model (CEM)
**When:** Basic features, clear requirements
**Focus:** Provide context, let AI generate

```markdown
## üéØ Context
**Stack:** Next.js 14, TypeScript, Supabase
**Patterns:** REST API, Server Components
**Constraints:** < 200ms response time

## üìå Active Work
- [ ] Create user profile endpoint
  - Acceptance: Returns user data with avatar
  - Constraint: Use Supabase RLS
```

### Level 2: Test-Driven Model (TDM)
**When:** Complex logic, need validation
**Focus:** Define tests first, AI implements to pass tests

```markdown
## üß™ Test Specifications

### Feature: Calculate Project Cost
**Given:** hours=100, rate=50, vat=20%
**When:** calculateCost(100, 50, 0.20)
**Then:** { total: 6000, vat: 1000, net: 5000 }

**Test Command:** `npm test tests/cost.test.ts`
**Coverage Target:** > 90%

## üìå Active Work
- [ ] Implement cost calculator
  - Acceptance: All tests pass
  - Test file exists: tests/cost.test.ts
```

**AI Prompt:**
```
Read PROJECT.md section "Test Specifications".
Implement code that makes all specified tests pass.
Follow patterns from "Code Patterns" section.
```

### Level 3: Planning-Driven Model (PDM)
**When:** Architecture-heavy, multiple modules
**Focus:** Define structure/interfaces first, then implement

```markdown
## üìê Architecture Plan

### Module: Authentication Service
```mermaid
graph TD
    A[API Gateway] --> B[Auth Service]
    B --> C[User Store]
    B --> D[Token Service]
```

**Interfaces:**
```typescript
interface AuthService {
  login(creds: Credentials): Promise<Token>
  verify(token: string): Promise<User>
  logout(token: string): Promise<void>
}

interface TokenService {
  generate(user: User): Promise<Token>
  validate(token: string): Promise<boolean>
}
```

## üìå Implementation Tasks
- [ ] Create auth service skeleton per interface
- [ ] Implement login method with bcrypt
- [ ] Add token generation with JWT
- [ ] Integrate with User Store
```

**AI Prompt:**
```
Check PROJECT.md "Architecture Plan".
Implement [ModuleName] according to interface.
Ensure integration with existing modules per diagram.
```

---

## Workflow Patterns

### Pattern 1: Test-Driven VibeCoding
```bash
# 1. Write test spec
echo "## Test: User Registration
Input: { email, password }
Expected: 201 status + user object
Test: tests/auth.test.ts" >> PROJECT.md

# 2. AI implements
"Read test spec in PROJECT.md and implement the feature"

# 3. Validate
npm test

# 4. Mark complete
sed -i '' 's/\[ \]/\[x\]/' PROJECT.md
```

### Pattern 2: Iterative Refinement (ICCM)
```markdown
## üîÅ Iteration Log

### Attempt 1
**Request:** "Create user CRUD endpoints"
**Result:** Missing input validation
**Feedback:** Add Zod schema validation

### Attempt 2
**Request:** "Add Zod validation for inputs"
**Result:** No error handling
**Feedback:** Add try-catch + proper error responses

### Attempt 3 ‚úÖ
**Request:** "Add comprehensive error handling"
**Result:** All tests pass, proper validation + errors
```

**Why it works:** AI learns from iteration history.

### Pattern 3: Context-Enhanced Generation
```markdown
## üé® Code Patterns

### Error Handling Pattern
```typescript
try {
  const result = await riskyOperation()
  return { success: true, data: result }
} catch (error) {
  if (error instanceof ValidationError) {
    return { success: false, error: error.message, code: 400 }
  }
  logger.error('Operation failed', error)
  return { success: false, error: 'Internal error', code: 500 }
}
```

### Naming Conventions
- Functions: camelCase, verb-first (getUserById, createOrder)
- Components: PascalCase (UserProfile, OrderList)
- Constants: UPPER_SNAKE_CASE (API_URL, MAX_RETRIES)
- Files: kebab-case (user-profile.tsx, order-list.tsx)
```

**Why it works:** AI learns project-specific patterns and applies consistently.

---

## Feedback Loops

### 1. Automated Test Feedback
```yaml
# .github/workflows/feedback.yml
on: [push]
jobs:
  test-feedback:
    steps:
      - run: npm test > test-results.txt
      - if: failure()
        run: |
          echo "## ‚ùå Test Failures" >> PROJECT.md
          cat test-results.txt >> PROJECT.md
```

### 2. Quality Gates
```markdown
## ‚úÖ Pre-Commit Checklist
- [ ] All tests pass (`npm test`)
- [ ] Coverage > 80% (`npm test -- --coverage`)
- [ ] No lint errors (`npm run lint`)
- [ ] Types valid (`npm run typecheck`)
- [ ] Build succeeds (`npm run build`)
```

### 3. Self-Debugging Loop
```bash
# Run tests, if fail, capture context for AI
npm test || {
  echo "## üêõ Debug Context" > debug.md
  echo "### Test Output:" >> debug.md
  npm test 2>&1 >> debug.md
  echo "AI: Analyze and fix failing tests"
}
```

---

## Safety Guardrails

### Security Rules (in PROJECT.md)
```markdown
## üîí Security Constraints

### NEVER
- Store passwords in plain text
- Commit API keys to git
- Skip input validation
- Use string concatenation for SQL

### ALWAYS
- Hash passwords with bcrypt (cost: 10)
- Validate input with Zod schemas
- Use parameterized queries
- Sanitize user uploads
- Rate limit APIs (100 req/min)
```

### Sandboxed Testing
```bash
# Test AI-generated code in isolation
docker run --rm -it \
  --memory="512m" \
  --cpus="0.5" \
  --network="none" \
  -v $(pwd):/code:ro \
  node:20 npm test
```

---

## Progressive Complexity

| Stage | PROJECT.md Size | Features |
|-------|----------------|----------|
| **Beginner** | ~100 lines | Basic context + tasks |
| **Intermediate** | ~300 lines | + Test specs + patterns |
| **Advanced** | ~500 lines | + Architecture + iteration logs |
| **Expert** | 500+ lines | + Multi-agent coordination |

---

## Enhanced PROJECT.md Template

```markdown
# Project: [Name]

## üéØ Context (CEM)
**Stack:** [tech]
**Patterns:** [architecture]
**Constraints:** [limits]

## üß™ Test Specifications (TDM)
### Feature: [Name]
**Given:** [precondition]
**When:** [action]
**Then:** [expected result]
**Test:** [test file path]

## üìê Architecture Plan (PDM)
[Diagram + Interfaces]

## üé® Code Patterns
[Project-specific patterns]

## üîÅ Iteration Log
[Learning from attempts]

## ‚úÖ Quality Gates
[Pre-commit checklist]

## üîí Security Rules
[Safety constraints]

## üìå Active Work
- [ ] Task with clear acceptance criteria
```

---

## AI Prompt Templates

### For TDM
```
You are implementing features test-first.

1. Read PROJECT.md "Test Specifications"
2. Implement code to pass ALL tests
3. Follow "Code Patterns" for consistency
4. Respect "Security Rules"

Do not modify tests. Make code pass tests.
```

### For PDM
```
You are implementing from architecture.

1. Read PROJECT.md "Architecture Plan"
2. Review interface definitions
3. Implement module per specification
4. Ensure integration with existing modules

Follow the interface contract exactly.
```

### For ICCM
```
You are refining previous attempts.

1. Read PROJECT.md "Iteration Log"
2. Understand previous feedback
3. Apply improvements based on learnings
4. Avoid repeating past mistakes

Build on what worked, fix what didn't.
```

---

## Key Takeaways

### Developer Role Shift
```
Old: Code writer
New: Context curator + quality judge
```

### AI Role
```
Old: Autocomplete
New: Code generator from context
```

### PROJECT.md Role
```
Old: Documentation
New: Context orchestration hub
```

---

## Integration Checklist

**Quick Start (5 min):**
- [ ] Add test specs to PROJECT.md
- [ ] Define code patterns
- [ ] Set quality gates

**Standard (30 min):**
- [ ] Choose model: TDM/PDM/ICCM
- [ ] Create prompt templates
- [ ] Setup feedback loops

**Advanced (2 hours):**
- [ ] Implement sandboxing
- [ ] Add CI/CD feedback
- [ ] Multi-agent coordination
